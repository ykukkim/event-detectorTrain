{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1urRLA8pbdO2vOO0Zb4eJD6WCkog27Mm_","authorship_tag":"ABX9TyN2xyr/VQkgWy5a5wdSwYhd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"wK0RhrUP0RWD","colab_type":"code","colab":{}},"source":["import os\n","import logging\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import pytorch_lightning as pl\n","import torch.optim as optimpip\n","import pdb\n","from torch.nn import functional as F\n","from pytorch_lightning import Trainer\n","from pathlib import Path\n","from torch.utils.data import DataLoader, IterableDataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DkTd_EFa0bek","colab_type":"code","colab":{}},"source":["class CoolDataset(IterableDataset):\n","    def __init__(self, dir_path: str, seq_length: int = 120, event_col=\"event_heelstrike\"):\n","        # this should go in the csv, make sure you change the open_csv then to automatically read headers\n","        self.colnames = [\"marker_{}\".format(i) for i in range(74)]\n","        self.colnames[-1] = \"event_heelstrike\"\n","        self.colnames[-2] = \"event_footoff\"\n","        \n","        self.input_cols = [\"marker_1\", \"marker_13\"]\n","        self.event_col = event_col\n","        super().__init__()\n","        self.files = tuple(Path(dir_path).glob(\"**/*.csv\"))\n","        assert seq_length % 2 == 0, \"Please pass an even seq length\"\n","        self.seq_length = seq_length\n","\n","    def __iter__(self):\n","        # what file and what event in that file have we processed last?\n","        self.file_nr = 0\n","        self.event_in_file = 0\n","        return self\n","\n","    def __next__(self):\n","        # can be cached, might be inefficient right now\n","        df = self.read_file(self.files[self.file_nr])\n","        events = df[df[self.event_col] == 1]\n","        if self.event_in_file >= len(events):\n","            logging.info(\"File is complete. Going to new file\")\n","            self.file_nr += 1\n","            if self.file_nr < len(self.files):\n","                self.event_in_file = 0\n","                return next(self)\n","            else:\n","                # processed the last file, we are done\n","                raise StopIteration\n","        else:\n","            # just get the next event in this file\n","            event_frame = events.iloc[self.event_in_file].name\n","            inp, out = self.sample_seq_around_event_frame(df, event_frame)\n","            self.event_in_file += 1\n","        return inp, out\n","\n","    def sample_seq_around_event_frame(self, df, event_idx):\n","        \"\"\"returns a dataframe of shape (self.seq_length, number of markers/input) + event_col\"\"\"\n","        random_comp = 5  # random.random() uniform random number between a and b\n","        low_idx = event_idx - self.seq_length // 2 + random_comp  # but make sure its not lower than 0\n","        high_idx = event_idx + self.seq_length // 2 + random_comp  # but make sure it s not higher than len of the series\n","        inp = df.iloc[low_idx:high_idx, :][self.input_cols]\n","        out = df.iloc[low_idx:high_idx][self.event_col]\n","        # assert inp.shape[0] == len(out) == self.seq_length\n","        return inp, out\n","\n","    def read_file(self, f):\n","        # asserts maybe\n","        df = pd.read_csv(open(f, \"r\"))\n","        df.columns = self.colnames\n","        return df\n","\n","    def count_events_per_file(self):\n","        return [(df[self.event_col] == 1).sum() for df in self.dataset]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sijZz7wE0nAM","colab_type":"code","colab":{}},"source":["class CoolSystem(pl.LightningModule):\n","\n","    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,num_layers=2):\n","        super(CoolSystem, self).__init__()\n","        # configs\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.batch_size = batch_size\n","        self.num_layers = num_layers\n","\n","        # model construct\n","        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n","        self.lstm = nn.LSTM(self.hidden_dim, output_dim)\n","\n","    def init_hidden(self):\n","        # This is what we'll initialise our hidden state as\n","        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n","                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n","\n","    def forward(self, input):\n","        # Forward pass through LSTM layer\n","        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n","        # shape of self.hidden: (a, b), where a and b both\n","        # have shape (num_layers, batch_size, hidden_dim).\n","        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n","\n","        # Only take the output from the final timetep\n","        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n","        y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n","        return y_pred.view(-1)\n","\n","    def training_step(self, batch, batch_idx):\n","        # REQUIRED\n","        x, y = batch\n","        y_hat = self.forward(x)\n","        loss = F.cross_entropy(y_hat, y)\n","        tensorboard_logs = {'train_loss': loss}\n","        return {'loss': loss, 'log': tensorboard_logs}\n","\n","    def test_step(self, batch, batch_idx):\n","        # OPTIONAL\n","        x, y = batch\n","        y_hat = self.forward(x)\n","        return {'test_loss': F.cross_entropy(y_hat, y)}\n","\n","    def configure_optimizers(self):\n","        # REQUIRED\n","        # can return multiple optimizers and learning_rate schedulers\n","        # (LBFGS it is automatically supported, no need for closure function)\n","        return torch.optim.Adam(self.parameters(), lr=0.02)\n","\n","    @pl.data_loader\n","    def train_dataloader(self):\n","        # REQUIRED\n","        return DataLoader(CoolDataset(Path(\"./train\")), batch_size=self.batch_size)\n","\n","    @pl.data_loader\n","    def test_dataloader(self):\n","        # OPTIONAL\n","        return DataLoader(CoolDataset(Path(\"./test\"), batch_size=self.batch_size))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wAKfgBRB0t05","colab_type":"code","colab":{}},"source":["if __name__ == '__main__':\n","    # ds = CoolDataset(r\"P:\\Projects\\NCM\\NCM_EXP\\NCM_STM\\NCM_HRX_Walking\")\n","    pdb.set_trace()\n","    ds = CoolDataset(r\"/content/drive/My\\ Drive/Colab\\ Notebooks/LMBTrain/data/csv/test\")\n","    for inp, out in ds:\n","        print(out[out == 1])\n","\n","    model = CoolSystem(30,4,32,1,3)\n","\n","    loss_fn = torch.nn.MSELoss(size_average=False)\n","    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    #####################\n","    # Train model\n","    #####################\n","\n","    hist = np.zeros(num_epochs)\n","\n","    for t in range(num_epochs):\n","        # Clear stored gradient\n","        model.zero_grad()\n","\n","        # Initialise hidden state\n","        # Don't do this if you want your LSTM to be stateful\n","        model.hidden = model.init_hidden()\n","\n","        # Forward pass\n","        y_pred = model(X_train)\n","\n","        loss = loss_fn(y_pred, y_train)\n","        if t % 100 == 0:\n","            print(\"Epoch \", t, \"MSE: \", loss.item())\n","        hist[t] = loss.item()\n","\n","        # Zero out gradient, else they will accumulate between epochs\n","        optimiser.zero_grad()\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Update parameters\n","        optimiser.step()\n","\n","    \"\"\""],"execution_count":0,"outputs":[]}]}